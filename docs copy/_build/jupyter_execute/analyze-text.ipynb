{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " :::{admonition} Development Process\n",
    " :class: important\n",
    "- Come up with an idea ðŸ˜€\n",
    "- Identify a dataset and specify precisely what you want to extract\n",
    "- Engineer a prompt (use OpenAI Playground)\n",
    "- Evaluate performance on a sample\n",
    "    * If performance is unacceptable, try further prompt engineering, functions, etc.\n",
    "    * If performance is still not good enough, try fine-tuning\n",
    "- Deploy\n",
    ":::\n",
    "\n",
    "```{figure} ./images/gpt-dev-cycle.png\n",
    "---\n",
    "width: 600px\n",
    "name: gpt-dev-cycle\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} An Idea\n",
    ":class: tip\n",
    "Identify clusters of people who are collaborating on Large Language Model research.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd346433c8b4218a52df73e1465e1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[20, 0], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoom_out_textâ€¦"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import leafmap\n",
    "map = leafmap.Map()\n",
    "# Need data! : map.add_xy_data(df, x=\"longitude\", y=\"latitude\", layer_name=\"World Cities\")\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Gather Data\n",
    ":class: tip\n",
    "Promising dataset: [arXiv](https://arxiv.org/)\n",
    "PDF files available, as well as [metadata](https://www.kaggle.com/datasets/Cornell-University/arxiv/)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Specify what information to extract\n",
    ":class: tip\n",
    "- title\n",
    "- list of author names\n",
    "- each author's email address\n",
    "- each author's affiliation\n",
    "- each affiliation's location in terms of latitude and longitude\n",
    ":::\n",
    "\n",
    ":::{card}\n",
    "```{figure} ./images/arxiv-paper-header.png\n",
    "---\n",
    "width: 600px\n",
    "name: arxiv-paper-header\n",
    "---\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{card}\n",
    "```json\n",
    "{ \"title\": \"The paper's title\",\n",
    "    \"authors\": [\n",
    "        {\n",
    "            \"name\": \"author's name\",\n",
    "            \"email\": \"name@domain.edu\",\n",
    "            \"affiliations\": [ \"list of indices\" ]\n",
    "        }\n",
    "    ],\n",
    "    \"affiliations\": [ \n",
    "        {\"index\": \"the index\", \n",
    "        \"name\": \"The affiliation name\", \n",
    "        \"longitude\": \"the longitude\", \n",
    "        \"latitude\": \"the latitude\" \n",
    "        } \n",
    "    ]\n",
    " ]\n",
    "}\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Develop a Prompt\n",
    "[OpenAI playground](https://platform.openai.com/playground)\n",
    ":::\n",
    "\n",
    ":::{card} Prompt engineering\n",
    "There are various prompting strategies you can use to improve performance. [OpenAI]([Prompt Engineering](https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions) has a very good guide to help you out. They also provide lots of examples to look at.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load the .env file containing your API key\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are an expert research librarian. You are precise and can analyze the structure of papers very well. You return information in json format.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Extract the title and authors and affiliations from the first page of a scientific paper. \\n\\nUse the following step-by-step instructions to respond to user inputs.\\n\\nExtract the title and authors from the first page of a scientific paper. The paper text will snipped will be delimited by triple quotes. Geolocate each author affiliation with latitude and longitude.\\n\\nThe output should have the following format:\\n\\n{ \\\"title\\\": \\\"The paper's title\\\",\\n  \\\"authors\\\": [\\n    {\\n      \\\"name\\\": \\\"Yong Ren\\\",\\n      \\\"email\\\": null,\\n      \\\"affiliations\\\": [ \\\"list of indices\\\" ]\\n    }\\n  ],\\n \\\"affiliations\\\": [ {\\\"index\\\": \\\"the index\\\", \\\"name\\\": \\\"The affiliation name\\\", \\\"longitude\\\": \\\"the longitude\\\", \\\"latitude\\\": \\\"the latitude\\\" } ]\\n ]\\n}\\n\\n\\\"\\\"\\\"\\nFEWER-TOKEN NEURAL SPEECH CODEC WITH TIME-INVARIANT CODES\\nYong Ren1,2, Tao Wang1, Jiangyan Yi1, Le Xu1,2, Jianhua Tao3, Chuyuan Zhang1,2, Junzuo Zhou1,2\\n1Institute of Automation, Chinese Academy of Sciences, China\\n2University of Chinese Academy of Sciences, China\\n3Department of Automation, Tsinghua University, China\\nABSTRACT\\nLanguage model based text-to-speech (TTS) models, like VALL-E,\\nhave gained attention for their outstanding in-context learning capa-\\nbility in zero-shot scenarios. Neural speech codec is a critical com-\\nponent of these models, which can convert speech into discrete token\\nrepresentations. However, excessive token sequences from the codec\\nmay negatively affect prediction accuracy and restrict the progres-\\nsion of Language model based TTS models. To address this issue,\\nthis paper proposes a novel neural speech codec with time-invariant\\ncodes named TiCodec. By encoding and quantizing time-invariant\\ninformation into a separate code, TiCodec can reduce the amount of\\nframe-level information that needs encoding, effectively decreasing\\nthe number of tokens as codes of speech. Furthermore, this paper\\nintroduces a time-invariant encoding consistency loss to enhance the\\nconsistency of time-invariant code within an utterance and force it\\nto capture more global information, which can benefit the zero-shot\\nTTS task. Experimental results demonstrate that TiCodec can not\\nonly enhance the quality of reconstruction speech with fewer tokens\\nbut also increase the similarity and naturalness, as well as reduce the\\nword error rate of the synthesized speech by the TTS model.\\nIndex Termsâ€” speech codec, fewer tokens, time-invariant, lan-\\nguage model, text-to-speech\\n\\\"\\\"\\\"\\n \"\n",
    "    }\n",
    "  ],\n",
    "  response_format={\"type\": \"json_object\"},\n",
    "  temperature=0,\n",
    "  max_tokens=2048,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"FEWER-TOKEN NEURAL SPEECH CODEC WITH TIME-INVARIANT CODES\",\n",
      "    \"authors\": [\n",
      "        {\n",
      "            \"name\": \"Yong Ren\",\n",
      "            \"email\": null,\n",
      "            \"affiliations\": [\n",
      "                1,\n",
      "                2\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Tao Wang\",\n",
      "            \"email\": null,\n",
      "            \"affiliations\": [\n",
      "                1\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Jiangyan Yi\",\n",
      "            \"email\": null,\n",
      "            \"affiliations\": [\n",
      "                1\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Le Xu\",\n",
      "            \"email\": null,\n",
      "            \"affiliations\": [\n",
      "                1,\n",
      "                2\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Jianhua Tao\",\n",
      "            \"email\": null,\n",
      "            \"affiliations\": [\n",
      "                3\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Chuyuan Zhang\",\n",
      "            \"email\": null,\n",
      "            \"affiliations\": [\n",
      "                1,\n",
      "                2\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Junzuo Zhou\",\n",
      "            \"email\": null,\n",
      "            \"affiliations\": [\n",
      "                1,\n",
      "                2\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"affiliations\": [\n",
      "        {\n",
      "            \"index\": 1,\n",
      "            \"name\": \"Institute of Automation, Chinese Academy of Sciences, China\",\n",
      "            \"longitude\": \"116.3975\",\n",
      "            \"latitude\": \"39.9085\"\n",
      "        },\n",
      "        {\n",
      "            \"index\": 2,\n",
      "            \"name\": \"University of Chinese Academy of Sciences, China\",\n",
      "            \"longitude\": \"116.6514\",\n",
      "            \"latitude\": \"40.1211\"\n",
      "        },\n",
      "        {\n",
      "            \"index\": 3,\n",
      "            \"name\": \"Department of Automation, Tsinghua University, China\",\n",
      "            \"longitude\": \"116.326\",\n",
      "            \"latitude\": \"40.0036\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = json.loads(response.choices[0].message.content)\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "output = \"\"\"\n",
    "{\n",
    "    \"title\": \"FEWER-TOKEN NEURAL SPEECH CODEC WITH TIME-INVARIANT CODES\",\n",
    "    \"authors\": [\n",
    "        {\n",
    "            \"name\": \"Yong Ren\",\n",
    "            \"email\": null,\n",
    "            \"affiliations\": [\n",
    "                1,\n",
    "                2\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Tao Wang\",\n",
    "            \"email\": null,\n",
    "            \"affiliations\": [\n",
    "                1\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Jiangyan Yi\",\n",
    "            \"email\": null,\n",
    "            \"affiliations\": [\n",
    "                1\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Le Xu\",\n",
    "            \"email\": null,\n",
    "            \"affiliations\": [\n",
    "                1,\n",
    "                2\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Jianhua Tao\",\n",
    "            \"email\": null,\n",
    "            \"affiliations\": [\n",
    "                3\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Chuyuan Zhang\",\n",
    "            \"email\": null,\n",
    "            \"affiliations\": [\n",
    "                1,\n",
    "                2\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Junzuo Zhou\",\n",
    "            \"email\": null,\n",
    "            \"affiliations\": [\n",
    "                1,\n",
    "                2\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"affiliations\": [\n",
    "        {\n",
    "            \"index\": 1,\n",
    "            \"name\": \"Institute of Automation, Chinese Academy of Sciences, China\",\n",
    "            \"longitude\": \"116.331398\",\n",
    "            \"latitude\": \"39.897445\"\n",
    "        },\n",
    "        {\n",
    "            \"index\": 2,\n",
    "            \"name\": \"University of Chinese Academy of Sciences, China\",\n",
    "            \"longitude\": \"116.651381\",\n",
    "            \"latitude\": \"40.12114\"\n",
    "        },\n",
    "        {\n",
    "            \"index\": 3,\n",
    "            \"name\": \"Department of Automation, Tsinghua University, China\",\n",
    "            \"longitude\": \"116.326443\",\n",
    "            \"latitude\": \"40.00368\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Evaluate on a sample\n",
    ":class: tip\n",
    "To evaluate the perfomance of the GPT on your dataset, you need some way of externally validating it. At least a portion of your data must be labelled with the correct (or at least likely correct) output. This is called a `gold standard`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "user_prompt_instructions = \"\"\"\n",
    "Extract the title and authors and affiliations from the first page of a scientific paper. \n",
    "\n",
    "Use the following step-by-step instructions to respond to user inputs.\n",
    "\n",
    "Extract the title and authors from the first page of a scientific paper. The paper text will snipped will be delimited by triple quotes. Geolocate each author affiliation with latitude and longitude.\n",
    "\n",
    "The output should have the following format:\n",
    "\n",
    "{ \"title\": \"The paper's title\",\n",
    "  \"authors\": [\n",
    "    {\n",
    "      \"name\": \"Yong Ren\",\n",
    "      \"email\": null,\n",
    "      \"affiliations\": [ \"list of indices\" ]\n",
    "    }\n",
    "  ],\n",
    " \"affiliations\": [ {\"index\": \"the index\", \"name\": \"The affiliation name\", \"longitude\": \"the longitude\", \"latitude\": \"the latitude\" } ]\n",
    " ]\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import openai\n",
    "\n",
    "def validate_response_data(data: Dict):\n",
    "    assert \"title\" in data, \"title not found\"\n",
    "    assert \"authors\" in data, \"title not found\"\n",
    "    for auth in data['authors']:\n",
    "        assert \"name\" in auth, \"name not found\"\n",
    "        assert \"email\" in auth, \"email not found\"\n",
    "        assert \"affiliations\" in auth, \"affiliations not found\"\n",
    "    assert \"affiliations\" in data, \"affiliations not found\"\n",
    "    for aff in data['affiliations']:\n",
    "        assert \"index\" in aff, \"index not found\"\n",
    "        assert \"name\" in aff, \"name not found\"\n",
    "        assert \"longitude\" in aff, \"longitude not found\"\n",
    "        assert \"latitude\" in aff, \"latitude not found\"\n",
    "\n",
    "def analyze_text(client: openai.Client, text: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert research librarian. You are precise and can analyze the structure of papers very well. You return information in json format.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt_instructions + '\\n\\n\"\"\"' + text + '\\n\\n\"\"\"'\n",
    "            }\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0,\n",
    "        max_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        seed=42,\n",
    "    )\n",
    "    try:\n",
    "        data = json.loads(response.choices[0].message.content)\n",
    "        print(data)\n",
    "        validate_response_data(data)\n",
    "        return json.dumps(data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "client = openai.Client()\n",
    "df_sample = df.sample(100, random_state=42)\n",
    "df_sample['extracted_info'] = df_sample['text'].apply(lambda x: analyze_text(client, x))\n",
    "df_sample.to_parquet(\"sample_output.parquet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_gold \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/arxiv_metadata.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df_extracted \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/extracted_data.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m true_positives \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm-env/lib/python3.11/site-packages/pandas/io/parquet.py:654\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    503\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    512\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    513\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    657\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm-env/lib/python3.11/site-packages/pandas/io/parquet.py:66\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     64\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_gold = pd.read_parquet('./data/arxiv_metadata.parquet')\n",
    "df_extracted = pd.read_parquet('./data/extracted_data.parquet')\n",
    "\n",
    "true_positives = []\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "\n",
    "for id in df_extracted['id']:\n",
    "    gold_authors = list(df_gold[df_gold['id'] == id]['authors'])[0]\n",
    "    gold_authors = {a.strip() for a in gold_authors.split(',')}\n",
    "\n",
    "    predicted = df_extracted[df_extracted['id'] == id]\n",
    "    predicted_authors = set(predicted['author'])\n",
    "    \n",
    "    for author in predicted_authors:\n",
    "        if author in gold_authors:\n",
    "            true_positives.append((id, author))\n",
    "        else:\n",
    "            false_positives.append((id, author))\n",
    "    \n",
    "    for author in gold_authors:\n",
    "        if author not in predicted_authors:\n",
    "            false_negatives.append((id, author))\n",
    "\n",
    "\n",
    "# round precision to 2 decimal places\n",
    "precision = round(len(true_positives) / (len(true_positives) + len(false_positives)), 2)\n",
    "\n",
    "# round recall to 2 decimal places\n",
    "recall = round(len(true_positives) / (len(true_positives) + len(false_negatives)), 2)\n",
    "\n",
    "print(f\"true_positives count: {len(true_positives)}\")\n",
    "print(f\"false_positives count: {len(false_positives)}\")\n",
    "print(f\"false_negatives count: {len(false_negatives)}\")\n",
    "print(\"precision:\", precision)\n",
    "print(\"recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positives for id 2310.08102\n",
      "\n",
      "  Ayu Purwarianti\n",
      "  Alham Fikri Aji\n",
      "  Muhammad Razif Rizqullah\n",
      "\n",
      "False negatives for id 2310.08102\n",
      "\n",
      "  Muhammad Razif Rizqullah (1)\n",
      "  (2) Mohamed bin Zayed University of\n",
      "  Artificial Intelligence)\n",
      "  Ayu Purwarianti (1) and Alham Fikri Aji\n",
      "  (2) ((1) Bandung Institute of Technology\n"
     ]
    }
   ],
   "source": [
    "fp_sample = {fp[1] for fp in false_positives if fp[0] == '2310.08102'}\n",
    "fn_sample = {fn[1] for fn in false_negatives if fn[0] == '2310.08102'}\n",
    "print(f\"False positives for id 2310.08102\\n\")\n",
    "for fp in fp_sample:\n",
    "    print(f\"  {fp}\")\n",
    "\n",
    "print(f\"\\nFalse negatives for id 2310.08102\\n\")\n",
    "for fn in fn_sample:\n",
    "    print(f\"  {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./images/arxiv-fp-fn-example.png\n",
    "---\n",
    "width: 600px\n",
    "name: arxiv-fp-fn-example\n",
    "---\n",
    "[Paper](https://arxiv.org/abs/2310.08102)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Deploy\n",
    ":class: tip\n",
    "Deployment of this code to the full dataset requires further pre-cautions:\n",
    "\n",
    "- Handle possible [errors](https://platform.openai.com/docs/guides/error-codes)\n",
    "- Respect [rate limits](https://platform.openai.com/docs/guides/rate-limits/rate-limits?context=tier-free)\n",
    "- Write incremental results to disk and generate a log\n",
    "- Ensure input arguments are correct\n",
    "\n",
    "\n",
    "```python\n",
    "@app.command()\n",
    "def complete_prompt(\n",
    "    data_file_path: Path = typer.Argument(..., help=\"Data file path name\"),\n",
    "    config_file_path: Path = typer.Argument(..., help=\"Config file path name\"),\n",
    "    outdir: Path = typer.Option(Path(\".\"), help=\"Output directory\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete a prompt in a data file and write output to a csv file.\n",
    "    \"\"\"\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Payoff\n",
    ":class: important\n",
    "Now we can do analysis on clusters of innovation and collaboration across authors and institutions. And we can populate the map!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c99dc7421149dd9968d9413aa634f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[42.0451, -87.6877], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zâ€¦"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import HTML\n",
    "from ipyleaflet import Map, Marker, Popup, MarkerCluster\n",
    "\n",
    "center = (42.0451, -87.6877)\n",
    "map2 = Map(center=center, zoom=2, close_popup_on_click=True)\n",
    "\n",
    "markers = []\n",
    "for row in list(df_extracted.iterrows())[:500]:\n",
    "    marker = Marker(location=(row[1]['latitude'], row[1]['longitude']))\n",
    "    message = HTML()\n",
    "    message.value = f\"{row[1]['author']}: <b>{row[1]['affiliation']}</b>\"\n",
    "    marker.popup = message\n",
    "    markers.append(marker)\n",
    "\n",
    "map2.add_layer(MarkerCluster(markers=markers))\n",
    "\n",
    "map2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}